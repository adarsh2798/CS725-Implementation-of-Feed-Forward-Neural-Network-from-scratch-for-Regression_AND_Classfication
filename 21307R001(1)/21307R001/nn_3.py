# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U0B1VNik5-cj3wfiVRIuff3BbkIMHP2g
"""

import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pandas as pd
df=pd.read_csv("https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/regression/data/train.csv",low_memory=False)
df_dev=pd.read_csv("https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/regression/data/dev.csv",low_memory=False)
df_test=pd.read_csv("https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/regression/data/test.csv",low_memory=False)
np.random.seed(42)

#NUM_FEATS = 90 #no. of neurons in input layer
mini_batch_size=64
class Net(object):
    def __init__(self,num_layers,num_units,NUM_FEATS):
            self.num_layers=num_layers
            self.num_units=num_units
            
            self.weights=[np.random.uniform(-1,1,(self.num_units,self.num_units)) for i in range(num_layers-1)]
            self.weights.insert(0,np.random.uniform(-1,1,(self.num_units,NUM_FEATS)))
            self.weights.append(np.random.uniform(-1,1,(1,self.num_units)))#output layer has only one neuron
            
            self.biases=[np.random.uniform(-1,1,(self.num_units,1)) for i in range(num_layers)]
            self.biases.append(np.random.uniform(-1,1,(1,1)))

            self.v_weights=[np.zeros((self.num_units,self.num_units)) for i in range(num_layers-1)]
            self.v_weights.insert(0,np.zeros((self.num_units,NUM_FEATS)))
            self.v_weights.append(np.zeros((1,self.num_units)))
            
            self.v_biases=[np.zeros((self.num_units,1)) for i in range(num_layers)]
            self.v_biases.append(np.zeros((1,1)))

            self.s_weights=[np.zeros((self.num_units,self.num_units)) for i in range(num_layers-1)]
            self.s_weights.insert(0,np.zeros((self.num_units,NUM_FEATS)))
            self.s_weights.append(np.zeros((1,self.num_units)))
            
            self.s_biases=[np.zeros((self.num_units,1)) for i in range(num_layers)]
            self.s_biases.append(np.zeros((1,1)))
    def __call__(self,X):
        
        a=np.array(X)#X is mxd array of 'm' trainig samples each of 'd' features
        a=a.T
        
        for (w,b) in (zip(self.weights[:-1],self.biases[:-1])):
            
                  h =reLU(np.matmul(w,a)+b)#we use broadcasting
                  a=h
        h=np.matmul(self.weights[-1],a)+self.biases[-1]#no activation in output layer
        return h.T
    def backward(self,X,Y,lamda):
          grad_w=[np.zeros(w.shape) for w in self.weights]
          grad_b=[np.zeros(b.shape) for b in self.biases]
          for x,y in zip(X,Y):
              x=x[np.newaxis].T
              single_grad_w=[np.zeros(w.shape) for w in self.weights]
              single_grad_b=[np.zeros(b.shape) for b in self.biases]
              ####### FORWARD PASS #######
              zs=[]
              A=[x]
              a=x
              for w,b in zip(self.weights[:-1],self.biases[:-1]):
                    z=np.matmul(w,a)+b
                    zs.append(z)
                    a=reLU(z)
                    A.append(a)
            
              zs.append(np.matmul(self.weights[-1],a)+self.biases[-1])
              A.append((zs[-1]))# no activation in output layer
              #print(len(A),len(zs))
              ####### BACKWARD PASS ######
              d_vu=[]
              d_Lv=[]
              d_Lu=[]
              for i in range(len(A)-1):
                    if i==0:
                        d_Lv.append(1)
                        d_vu.append(loss_mse_deriv(A[-1],y))
                        d_Lu.append(np.dot(d_vu[0],d_Lv[0]))
                        single_grad_w[-1]=np.matmul(d_Lu[0],A[-2].transpose())+self.weights[-1]*lamda
                        single_grad_b[-1]=d_Lu[0]+self.biases[-1]*lamda
                    if i==1:
                        d_Lv.insert(0,d_Lu[0])
                        d_vu.insert(0,self.weights[-i].transpose())
                        #print(d_vu[0].shape,d_Lv[0].shape)
                        d_Lu.insert(0,np.matmul(d_vu[0],d_Lv[0]))
                        single_grad_w[-i-1]=np.multiply(np.matmul(d_Lu[0],A[-i-2].transpose()),dreLU(zs[-i-1]))+self.weights[-i-1]*lamda
                        single_grad_b[-i-1]=np.multiply(d_Lu[0],dreLU(zs[-i-1]))+self.biases[-i-1]*lamda
                    if(i>1):
                        #print(i)
                        d_Lv.insert(0,d_Lu[0])
                        d_vu.insert(0,self.weights[-i].transpose())
                        #print(d_vu[0].shape,d_Lv[0].shape)
                        d_Lu.insert(0,np.multiply(np.matmul(d_vu[0],d_Lv[0]),dreLU(zs[-i])))
                        single_grad_w[-i-1]=np.multiply(np.matmul(d_Lu[0],A[-i-2].transpose()),dreLU(zs[-i-1]))+self.weights[-i-1]*lamda
                        single_grad_b[-i-1]=np.multiply(d_Lu[0],dreLU(zs[-i-1]))+self.biases[-i-1]*lamda
                        
              grad_w=[gw+sgw for gw,sgw in zip(grad_w,single_grad_w)]
              grad_b=[gb+sgb for gb,sgb in zip(grad_b,single_grad_b)]
          return(grad_w,grad_b)  
class Optimizer(object):


    def __init__(self, learning_rate,beta,gamma):
         self.lr=learning_rate
         self.beta=beta
         self.gamma=gamma
         self.bt=1
         self.gt=1
    def step(self, weights, biases,v_weights,v_biases, s_weights,s_biases,delta_weights, delta_biases):
        v_weights=[self.beta*vw +(1-self.beta)*dw for vw,dw in zip(v_weights,delta_weights)]
        v_biases=[self.beta*vb +(1-self.beta)*db for vb,db in zip(v_biases,delta_biases)]
        s_weights=[self.gamma*sw +(1-self.gamma)*dw*dw for sw,dw in zip(s_weights,delta_weights)]
        s_biases=[self.gamma*sb +(1-self.gamma)*db*db for sb,db in zip(s_biases,delta_biases)]
        self.bt*=self.beta
        self.gt*=self.gamma
        v_hat_weights=[vw*(1/(1-self.bt)) for vw in v_weights]
        v_hat_biases=[vb*(1/(1-self.bt)) for vb in v_biases]
        s_hat_weights=[sw*(1/(1-self.gt)) for sw in s_weights]
        s_hat_biases=[sb*(1/(1-self.gt)) for sb in s_biases]
        lr_custom_weights=[self.lr*vhw/(np.sqrt(shw)+1e-7) for vhw,shw in zip(v_hat_weights,s_hat_weights)]
        lr_custom_biases=[self.lr*vhb/(np.sqrt(shb)+1e-7) for vhb,shb in zip(v_hat_biases,s_hat_biases)]
        weights=[w-((lrcw/mini_batch_size)) for w,lrcw in zip(weights,lr_custom_weights)]
        biases=[b-((lrcb/mini_batch_size)) for b,lrcb in zip(biases,lr_custom_biases)]
        return (weights,biases,v_weights,v_biases,s_weights,s_biases)


def PCA(train_input,dev_input,test_input,n):
    mean=train_input.mean(axis=0)
    B=train_input-np.matmul(np.ones((train_input.shape[0],1)),mean[np.newaxis])
    C=np.matmul(B.T,B)/(B.shape[0]-1)
    w,v=np.linalg.eig(C)
    l=list(zip(w,v.T))
    l=sorted(l,key=lambda x:x[0],reverse=True)  
    w=np.sort(w)[::-1]
    ss=0
    

    V=[]
    for i in range(n):
        V.append(l[i][1])


    V=np.array(V).T
    np.savetxt(r'/content/BOOK.csv', (V), delimiter=',')
    print(V)
    M_train=np.matmul(B,V)
    M_dev=np.matmul(dev_input,V)
    M_test=np.matmul(test_input,V)
    return M_train,M_dev,M_test

def param_std(x):
      return x.mean(),x.std()
def stdz(x,m,s):
  return   (x-m)/s             
def reLU(x):
    return np.maximum(x,np.zeros(x.shape))
def dreLU(x):
    #return (x>0)*1
    return (x>np.zeros(x.shape)).astype(int)
def softmax(x):
    x_soft=np.zeros(x.shape)
    base=np.sum(np.exp(x))
    x_soft=np.exp(x)/base
    return x_soft
def loss_mse(y,y_hat):
    #print((np.linalg.norm(y-y_hat)))
    return (np.linalg.norm(y-y_hat))**2/(2*mini_batch_size)
def loss_regularization(lamda,weights,biases):
  return (lamda/2)*(np.sum([np.sum(i*i) for i in weights])+np.sum([np.sum(i*i) for i in biases]))
def loss_fn(mse,reg):
  return(mse+reg)
def rmse(mse):
  return (mse**0.5)

def loss_mse_deriv(y_i,y_hat_i):
    return (y_i-y_hat_i)/mini_batch_size

def read_data():
    
    train_input=df.iloc[:,1:].to_numpy()
    train_target=df.iloc[:,0].to_numpy()
    dev_input=df_dev.iloc[:,1:].to_numpy()
    dev_target=df_dev.iloc[:,0].to_numpy()
    test_input=df_test.iloc[:,0:].to_numpy()
    train_input,dev_input,test_input=PCA(train_input,dev_input,test_input,5)
    for i in range(train_input.shape[1]):
                m,s=param_std(train_input[:,i])
                train_input[:,i]=stdz(train_input[:,i],m,s)
                dev_input[:,i]=stdz(dev_input[:,i],m,s)
                test_input[:,i]=stdz(test_input[:,i],m,s)
    
    NUM_FEATS=train_input.shape[1]
    print(NUM_FEATS)
    return (train_input,train_target,dev_input,dev_target,test_input,NUM_FEATS)
def train(net, optimizer, lamda, batch_size, max_epochs,train_input, train_target,dev_input,dev_target):
    m=train_input.shape[0]
    train_reg_mse_loss_arr=[]
    dev_rmse_loss_arr=[]
    epoch_arr=[]
    
    for e in range(max_epochs):
        epoch_loss=0
        for i in range(0,m,batch_size):
            batch_input=train_input[i:i+batch_size]
            batch_target=train_target[i:i+batch_size][np.newaxis].T
            #pred=net(batch_input)
            dw,db=net.backward(batch_input,batch_target,lamda)
            weights_updated,biases_updated,v_weights_updated,v_biases_updated,s_weights_updated,s_biases_updated=optimizer.step(net.weights,net.biases,net.v_weights,net.v_biases,net.s_weights,net.s_biases,dw,db)
            net.weights=weights_updated
            net.biases=biases_updated
            net.v_weights=v_weights_updated
            net.v_biases=v_biases_updated
            net.s_weights=s_weights_updated
            net.s_biases=s_biases_updated
            #mse=loss_mse(batch_target,pred)
            #reg=loss_regularization(lamda,net.weights,net.biases)
            #batch_loss=loss_fn(mse,reg)
            
            #epoch_loss+=batch_loss
        train_pred=net(train_input)
        mse=loss_mse(train_target[np.newaxis].T,train_pred)/1275.0
        reg=loss_regularization(lamda,net.weights,net.biases)
        train_loss=loss_fn(mse,reg)
        dev_pred=net(dev_input)
        mse=loss_mse(dev_target[np.newaxis].T,dev_pred)*mini_batch_size/dev_target[np.newaxis].T.shape[0]
        dev_loss=rmse(mse)
        
        train_reg_mse_loss_arr.append(train_loss)
        dev_rmse_loss_arr.append(dev_loss)
        epoch_arr.append(e+1)
        print(e,train_loss,"dev loss:",dev_loss)
    return(epoch_arr,train_reg_mse_loss_arr,dev_rmse_loss_arr)
def main():
    max_epochs=100
    batch_size=64
    learning_rate=0.1
    num_layers=2
    num_units=64
    lamda=0.1
    beta=0.9
    gamma=0.999
    train_input,train_target,dev_input,dev_target,test_input,NUM_FEATS=read_data()
    net=Net(num_layers,num_units,NUM_FEATS)
    optimizer=Optimizer(learning_rate,beta,gamma)
    epoch_arr,train_reg_mse_loss_arr,dev_rmse_loss_arr=train(net, optimizer, lamda, batch_size, max_epochs,train_input, train_target,dev_input,dev_target)
    #plt.plot(epoch_arr,train_reg_mse_loss_arr,color="blue")
    plt.plot(epoch_arr,dev_rmse_loss_arr,color="red")
    plt.show()
    test_pred=net(test_input)
    
    
    np.savetxt(r'/content/Book1_L.csv', (test_pred), delimiter=',')
                                
                
if __name__=='__main__':
    main()