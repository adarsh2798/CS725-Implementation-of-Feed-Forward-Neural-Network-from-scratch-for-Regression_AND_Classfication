# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U0B1VNik5-cj3wfiVRIuff3BbkIMHP2g
"""

import sys
import os
import numpy as np
import pandas as pd
import numpy.ma as ma
df=pd.read_csv("https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/classification/data/train.csv", low_memory=False)
df_dev=pd.read_csv("https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/classification/data/dev.csv", low_memory=False)
df_test=pd.read_csv("https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/classification/data/test.csv",low_memory=False)
np.random.seed(42)

#NUM_FEATS = 90 #no. of neurons in input layer
mini_batch_size=32
class Net(object):
    def __init__(self,num_layers,num_units,NUM_FEATS):
            self.num_layers=num_layers
            self.num_units=num_units
            print(NUM_FEATS)
            self.weights=[np.random.uniform(-1,1,(self.num_units,self.num_units)) for i in range(num_layers-1)]
            self.weights.insert(0,np.random.uniform(-1,1,(self.num_units,NUM_FEATS)))
            self.weights.append(np.random.uniform(-1,1,(4,self.num_units)))#output layer has only 4 neuron
            
            self.biases=[np.random.uniform(-1,1,(self.num_units,1)) for i in range(num_layers)]
            self.biases.append(np.random.uniform(-1,1,(4,1)))
            
            self.v_weights=[np.zeros((self.num_units,self.num_units)) for i in range(num_layers-1)]
            self.v_weights.insert(0,np.zeros((self.num_units,NUM_FEATS)))
            self.v_weights.append(np.zeros((4,self.num_units)))
            
            self.v_biases=[np.zeros((self.num_units,1)) for i in range(num_layers)]
            self.v_biases.append(np.zeros((4,1)))

            self.s_weights=[np.zeros((self.num_units,self.num_units)) for i in range(num_layers-1)]
            self.s_weights.insert(0,np.zeros((self.num_units,NUM_FEATS)))
            self.s_weights.append(np.zeros((4,self.num_units)))
            
            self.s_biases=[np.zeros((self.num_units,1)) for i in range(num_layers)]
            self.s_biases.append(np.zeros((4,1)))
    def __call__(self,X,flag):
        
        a=np.array(X)#X is mxd array of 'm' trainig samples each of 'd' features
        

        a=a.T
        if(flag==0):
                 for i in range(a.shape[0]):
                                a[i,:]=stdz(a[i,:])
        
        for (w,b) in (zip(self.weights[:-1],self.biases[:-1])):
                  
                  h =reLU(np.matmul(w,a)+b)#we use broadcasting
                  a=h
        #print(self.weights[-1].shape,a.shape)
        h=np.matmul(self.weights[-1],a)+self.biases[-1]#no activation in output layer
        h=sigmoid(h)
        return h.T
    def backward(self,X,Y,lamda):
          #print(Y.shape) 
          grad_w=[np.zeros(w.shape) for w in self.weights]
          grad_b=[np.zeros(b.shape) for b in self.biases]
          for x,y in zip(X,Y):
              #print(y)
              
              x=x[np.newaxis].T
              single_grad_w=[np.zeros(w.shape) for w in self.weights]
              single_grad_b=[np.zeros(b.shape) for b in self.biases]
              ####### FORWARD PASS #######
              zs=[]
              A=[x]
              a=x
              for w,b in zip(self.weights[:-1],self.biases[:-1]):
                    z=np.matmul(w,a)+b
                    zs.append(z)
                    a=reLU(z)
                    
                    A.append(a)
            
              zs.append(np.matmul(self.weights[-1],a)+self.biases[-1])
              A.append((sigmoid(zs[-1])))# no activation in output layer
              #print(len(A),len(zs))
              ####### BACKWARD PASS ######
              d_vu=[]
              d_Lv=[]
              d_Lu=[]
              for i in range(len(A)-1):
                    if i==0:
                        d_Lv.append(1)
                        d_vu.append(loss_mse_deriv(y[np.newaxis].T,A[-1]))
                        d_Lu.append(np.dot(d_vu[0],d_Lv[0]))
                        #print(d_vu[0].shape,d_Lu[0].shape,A[-2].transpose().shape)
                        single_grad_w[-1]=np.multiply(np.matmul(d_Lu[0],A[-2].transpose()),sigmoid_prime(zs[-i-1]))
                        single_grad_b[-1]=np.multiply(d_Lu[0],sigmoid_prime(zs[-i-1]))
                    if i==1:
                        d_Lv.insert(0,d_Lu[0])
                        d_vu.insert(0,np.multiply(self.weights[-i].transpose(),sigmoid_prime(zs[-i]).T))
                        #print(d_vu[0].shape,d_Lv[0].shape)
                        d_Lu.insert(0,np.matmul(d_vu[0],d_Lv[0]))
                        single_grad_w[-i-1]=np.multiply(np.matmul(d_Lu[0],A[-i-2].transpose()),dreLU(zs[-i-1]))
                        single_grad_b[-i-1]=np.multiply(d_Lu[0],dreLU(zs[-i-1]))
                    if(i>1):
                        #print(i)
                        d_Lv.insert(0,d_Lu[0])
                        d_vu.insert(0,self.weights[-i].transpose())
                        #print(d_vu[0].shape,d_Lv[0].shape)
                        d_Lu.insert(0,np.multiply(np.matmul(d_vu[0],d_Lv[0]),dreLU(zs[-i])))
                        single_grad_w[-i-1]=np.multiply(np.matmul(d_Lu[0],A[-i-2].transpose()),dreLU(zs[-i-1]))
                        single_grad_b[-i-1]=np.multiply(d_Lu[0],dreLU(zs[-i-1]))
                        
              grad_w=[gw+sgw for gw,sgw in zip(grad_w,single_grad_w)]
              grad_b=[gb+sgb for gb,sgb in zip(grad_b,single_grad_b)]
          return(grad_w,grad_b)  
class Optimizer(object):


    def __init__(self, learning_rate,beta,gamma):
         self.lr=learning_rate
         self.beta=beta
         self.gamma=gamma
         self.bt=1
         self.gt=1
    def step(self, weights, biases,v_weights,v_biases, s_weights,s_biases,delta_weights, delta_biases):
        v_weights=[self.beta*vw +(1-self.beta)*dw for vw,dw in zip(v_weights,delta_weights)]
        v_biases=[self.beta*vb +(1-self.beta)*db for vb,db in zip(v_biases,delta_biases)]
        s_weights=[self.gamma*sw +(1-self.gamma)*dw*dw for sw,dw in zip(s_weights,delta_weights)]
        s_biases=[self.gamma*sb +(1-self.gamma)*db*db for sb,db in zip(s_biases,delta_biases)]
        self.bt*=self.beta
        self.gt*=self.gamma
        v_hat_weights=[vw*(1/(1-self.bt)) for vw in v_weights]
        v_hat_biases=[vb*(1/(1-self.bt)) for vb in v_biases]
        s_hat_weights=[sw*(1/(1-self.gt)) for sw in s_weights]
        s_hat_biases=[sb*(1/(1-self.gt)) for sb in s_biases]
        lr_custom_weights=[self.lr*vhw/(np.sqrt(shw)+1e-8) for vhw,shw in zip(v_hat_weights,s_hat_weights)]
        lr_custom_biases=[self.lr*vhb/(np.sqrt(shb)+1e-8) for vhb,shb in zip(v_hat_biases,s_hat_biases)]
        weights=[w-((lrcw/mini_batch_size)) for w,lrcw in zip(weights,lr_custom_weights)]
        biases=[b-((lrcb/mini_batch_size)) for b,lrcb in zip(biases,lr_custom_biases)]
        return (weights,biases,v_weights,v_biases,s_weights,s_biases)
def PCA(train_input,dev_input,test_input):
    mean=train_input.mean(axis=0)
    B=train_input-np.matmul(np.ones((train_input.shape[0],1)),mean[np.newaxis])
    C=np.matmul(B.T,B)/(B.shape[0]-1)
    w,v=np.linalg.eig(C)
    l=list(zip(w,v.T))
    l=sorted(l,key=lambda x:x[0],reverse=True)  
    w=np.sort(w)[::-1]
    ss=0
    for i in range(len(w)):
      ss+=w[i]
      if(ss>0.92):
        index=i
        break

    V=[]
    for i in range(index+1):
        V.append(l[i][1])


    V=np.array(V).T

    M_train=np.matmul(B,V)
    M_dev=np.matmul(dev_input,V)
    M_test=np.matmul(test_input,V)
    return M_train,M_dev,M_test



def param_std(x):
      return x.mean(),x.std()
def stdz(x,m,s):
  return   (x-m)/s   
def param_scale(x):
     return x.max(),x.min()
def scale(x,max,min):
     return x-min/(max-min)            
def reLU(x):
    return np.maximum(x,np.zeros(x.shape))
def dreLU(x):
    #return (x>0)*1
    return (x>np.zeros(x.shape)).astype(int)
def softmax(x):
    
    base=np.sum(np.exp(x),axis=0)
    #print(base)
    x_soft=np.multiply(np.exp(x) ,1./base)
    #print(np.exp(x))
    return x_soft
def jac_softmax(x):
  x=x[np.newaxis].T
  x=softmax(x)
  #print(x)
  ones=np.ones((1,x.shape[0]))
  s=np.matmul(x,ones)
  
  return(np.multiply(s,(np.identity(x.shape[0])-s.T)))
def sigmoid(z):
    """The sigmoid function."""
    return 1.0/(1.0+np.exp(-z))
def sigmoid_prime(z):
    """Derivative of the sigmoid function."""
    return sigmoid(z)*(1-sigmoid(z))
ind=0       
def loss_mse(y,y_hat):
    #print((np.linalg.norm(y-y_hat)))
    return (np.sum((y-y_hat)*(y-y_hat)))/(2*mini_batch_size)
def loss_mse_deriv(y_i,y_hat_i):
    return (-y_i+y_hat_i)/mini_batch_size
def cross_ent(y,y_hat):
  #print(np.log(y_hat))
  #print(np.sum(np.multiply(y,ma.log(y_hat))))
  return(np.sum(np.multiply(y,ma.log(y_hat))))
def cross_ent_deriv(y,y_hat):
     
     return(np.multiply(-1*y,1./y_hat))

def read_data():
    
    train_input=df.iloc[:,1:].to_numpy()
    train_target=df.iloc[:,0].to_numpy()
    
    dev_input=df_dev.iloc[:,1:].to_numpy()
    dev_target=df_dev.iloc[:,0].to_numpy()
    test_input=df_test.iloc[:,0:].to_numpy()
    #train_input,dev_input,test_input=PCA(train_input,dev_input,test_input)
    Dict={"Recent":np.array([1,0,0,0]).astype(float),"Very Old":np.array([0,1,0,0]).astype(float),
          "Old":np.array([0,0,1,0]).astype(float),"New":np.array([0,0,0,1]).astype(float)}
    l_train_target=[]
    l_dev_target=[]
    for t in train_target:
        l_train_target.append(Dict[t])
    l_train_target=np.array(l_train_target)
    
    for t in dev_target:
        l_dev_target.append(Dict[t])
    l_dev_target=np.array(l_dev_target)
    print(train_input.shape)
    NUM_FEATS=train_input.shape[1]
    for i in range(train_input.shape[1]):
                m,s=param_std(train_input[:,i])
                train_input[:,i]=stdz(train_input[:,i],m,s)
                dev_input[:,i]=stdz(dev_input[:,i],m,s)
                test_input[:,i]=stdz(test_input[:,i],m,s)
                max,min=param_scale(train_input[:,i])
                train_input[:,i]=scale(train_input[:,i],max,min)
                dev_input[:,i]=scale(dev_input[:,i],max,min)
                test_input[:,i]=scale(test_input[:,i],max,min)

    
    
    return (train_input,l_train_target,dev_input,l_dev_target,test_input,NUM_FEATS)
def train(net, optimizer, lamda, batch_size, max_epochs,train_input, train_target,dev_input,dev_target):
    m=train_input.shape[0]
    for e in range(max_epochs):
        epoch_loss=0
        r=0
        for i in range(0,m,batch_size):
            #print(net.weights)
            batch_input=train_input[i:i+batch_size]
            batch_target=train_target[i:i+batch_size]
            pred=net(batch_input,1)
            dw,db=net.backward(batch_input,batch_target,lamda)
            weights_updated,biases_updated,v_weights_updated,v_biases_updated,s_weights_updated,s_biases_updated=optimizer.step(net.weights,net.biases,net.v_weights,net.v_biases,net.s_weights,net.s_biases,dw,db)
            net.weights=weights_updated
            net.biases=biases_updated
            net.v_weights=v_weights_updated
            net.v_biases=v_biases_updated
            net.s_weights=s_weights_updated
            net.s_biases=s_biases_updated
            
            batch_loss=loss_mse(batch_target,pred)
            
            
            
            epoch_loss+=batch_loss
            r+=1
            #print(r)
        #print(batch_target,pred)
        print("************************************************")
        dev_loss=loss_mse(dev_target,net(dev_input,1))
        #print(train_target[40794:40799],pred[26:31])
        print(e,epoch_loss,"dev loss",dev_loss)
        
        #train_pred=net(train_input[0:1000])
    #print(batch_target,pred)    
    #dev_pred=net(dev_input)
    #dev_rmse=(loss_mse(dev_target,dev_pred)*mini_batch_size/dev_pred.shape[0])**0.5
    #print("dev rms:",dev_rmse,dev_pred.shape[0])
    #print(dev_target,net(dev_input))
def main():
    max_epochs=20
    batch_size=32
    learning_rate=1e-3
    beta=0.85
    gamma=0.999
    num_layers=2
    num_units=20
    lamda=0.1
    train_input,train_target,dev_input,dev_target,test_input,NUM_FEATS=read_data()
    net=Net(num_layers,num_units,NUM_FEATS)
    optimizer=Optimizer(learning_rate,beta,gamma)
    train(net, optimizer, lamda, batch_size, max_epochs,train_input, train_target,dev_input,dev_target)
    print("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
    
    test_pred=net(test_input,1)
    
    index=np.argmax(test_pred,axis=1)
    print(test_pred[0:4])
    index_edit=[]
    print(index)
    for i in range(index[np.newaxis].shape[1]):
      if(index[i]==0):
        index_edit.append("Recent")
      if(index[i]==1):
        index_edit.append("Very Old")
      if(index[i]==2):
        index_edit.append("Old")
      if(index[i]==3):
        index_edit.append("New")
    index_edit=np.array(index_edit)[np.newaxis].T
    print(index_edit)
    np.savetxt(r'/content/Book1.csv', (index_edit), delimiter=',',fmt="%s")
                                
                
if __name__=='__main__':
    main()